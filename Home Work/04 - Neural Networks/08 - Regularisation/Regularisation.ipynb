{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c267720-cfc1-468e-939b-5b2775531e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is regularization in the context of deep learning? Why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10a2e6-3956-43a2-8665-42dec7a4d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Regularization is a crucial concept in the context of deep learning, as it helps to address the issue of overfitting and improve the generalization performance of neural networks.\n",
    "\n",
    "Overfitting in Deep Learning:\n",
    "\n",
    "Deep neural networks have a large number of parameters and can often fit the training data very well, even to the point of memorizing the training examples.\n",
    "This can lead to high training performance but poor performance on new, unseen data, a phenomenon known as overfitting.\n",
    "Overfitting occurs when the model is too complex and captures the noise or idiosyncrasies in the training data, rather than the underlying patterns.\n",
    "Importance of Regularization:\n",
    "\n",
    "Improving Generalization:\n",
    "Regularization techniques help to prevent overfitting and improve the generalization performance of the neural network, allowing it to perform well on new, unseen data.\n",
    "By constraining the model's complexity or introducing additional constraints, regularization techniques encourage the model to learn more robust and generalizable features.\n",
    "Reducing Complexity:\n",
    "Regularization can help to control the complexity of the neural network, preventing it from becoming too expressive and capturing unnecessary details in the training data.\n",
    "This can lead to a more efficient and interpretable model, as the network is forced to focus on the most relevant features for the task at hand.\n",
    "Stabilizing Training:\n",
    "Regularization can also help to stabilize the training process, making it less sensitive to the initialization of the model parameters and the choice of hyperparameters.\n",
    "This can be particularly important in the context of deep learning, where the training process can be challenging and prone to instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769a89e-7d36-4af0-bfa3-2d05820d1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12893af1-6863-4944-9775-a8df13666314",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between a model's ability to fit the training data (low bias) and its ability to generalize to new, unseen data (low variance). Regularization plays a crucial role in addressing this tradeoff.\n",
    "\n",
    "Bias-Variance Tradeoff:\n",
    "Bias refers to the error introduced by the model's simplifying assumptions, which can lead to systematic errors in the predictions.\n",
    "Variance refers to the sensitivity of the model's predictions to the training data, which can lead to overfitting and poor generalization.\n",
    "There is an inherent tradeoff between bias and variance: models with high complexity (low bias) tend to have high variance, while simpler models (high bias) tend to have low variance.\n",
    "Implications of the Bias-Variance Tradeoff:\n",
    "Underfitting: A model with high bias and low variance will underfit the training data, resulting in poor performance on both the training and test sets.\n",
    "Overfitting: A model with low bias and high variance will overfit the training data, performing well on the training set but poorly on the test set.\n",
    "Generalization: The goal is to find the right balance between bias and variance to achieve good generalization performance on new, unseen data.\n",
    "Role of Regularization in Addressing the Tradeoff:\n",
    "Regularization techniques help to address the bias-variance tradeoff by controlling the complexity of the model and preventing overfitting.\n",
    "By introducing a penalty term or additional constraints, regularization techniques discourage the model from learning complex patterns that may not generalize well.\n",
    "How Regularization Helps:\n",
    "L1 and L2 Regularization (Weight Decay):\n",
    "These techniques add a penalty term to the loss function, encouraging the model to learn smaller, more sparse weights.\n",
    "This helps to reduce the model's complexity, leading to a higher bias but lower variance, and improved generalization.\n",
    "Dropout:\n",
    "Dropout randomly deactivates a subset of the neurons during training, forcing the model to learn more robust and redundant features.\n",
    "This reduces the model's sensitivity to the training data, leading to lower variance and better generalization.\n",
    "Batch Normalization:\n",
    "Batch normalization helps to stabilize the training process and reduce the internal covariate shift, which can lead to more stable and generalizable representations.\n",
    "Data Augmentation:\n",
    "By artificially expanding the training dataset, data augmentation increases the diversity of the training data, reducing the model's sensitivity to the specific training examples.\n",
    "This helps to lower the variance and improve the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6479040-e984-4b83-9364-7f87e0fc9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and\n",
    "their effects on the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa05251-f28c-4d35-a4cc-1ab03ada5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. L1 and L2 regularization are two common regularization techniques used in machine learning, \n",
    "particularly in the context of deep learning. They differ in the way they calculate the penalty term \n",
    "and the effects they have on the model.\n",
    "    \n",
    "Differences between L1 and L2 Regularization:\n",
    "\n",
    "Penalty Calculation:\n",
    "L1 regularization uses the absolute value of the parameters, while L2 regularization uses the squared value of the parameters.\n",
    "Sparsity:\n",
    "L1 regularization tends to produce sparse models, where some parameters are exactly zero, resulting in feature selection.\n",
    "L2 regularization produces models with smaller, but non-zero, parameter values, without necessarily driving them to zero.\n",
    "Effect on the Model:\n",
    "L1 regularization can lead to more interpretable models, as it encourages feature selection and sparse representations.\n",
    "L2 regularization tends to produce models that are more stable and less sensitive to individual features, which can be beneficial for generalization.\n",
    "Optimization Behavior:\n",
    "The L1 penalty is non-differentiable at zero, which can lead to more complex optimization challenges, especially for gradient-based methods.\n",
    "The L2 penalty is differentiable, making it easier to optimize using standard gradient-based techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7b0c2-d1d2-4fc2-8eef-c6d07f69fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep\n",
    "learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286023ef-f455-4f4b-8cdb-a4fded0a9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize to new, unseen data. Regularization techniques help address this issue by introducing additional constraints or penalties to the model, encouraging it to learn more robust and generalizable features.\n",
    "\n",
    "The importance of regularization in deep learning can be attributed to the following:\n",
    "\n",
    "Model Complexity and Overfitting:\n",
    "Deep neural networks have a large number of parameters, which makes them highly expressive and capable of fitting complex patterns in the training data.\n",
    "Without proper regularization, deep models can easily overfit the training data, memorizing the noise and idiosyncrasies, rather than learning the underlying patterns.\n",
    "Regularization techniques help control the model's complexity, preventing it from becoming too expressive and overfit.\n",
    "Improving Generalization:\n",
    "Regularization encourages the model to learn more generalizable features by introducing additional constraints or penalties.\n",
    "This forces the model to focus on the most relevant features for the task, rather than capturing spurious correlations in the training data.\n",
    "By improving the model's ability to generalize, regularization techniques help ensure that the model performs well on new, unseen data, which is the ultimate goal of machine learning.\n",
    "Common Regularization Techniques:\n",
    "L1 and L2 Regularization (Weight Decay): These techniques add a penalty term to the loss function, proportional to the absolute (L1) or squared (L2) values of the model parameters. This encourages the model to learn smaller, more sparse weights.\n",
    "Dropout: Dropout randomly deactivates a subset of the neurons during training, forcing the model to learn more robust and redundant features.\n",
    "Batch Normalization: Batch normalization helps reduce the internal covariate shift, making the training process more stable and reducing the need for careful initialization and tuning of other hyperparameters.\n",
    "Data Augmentation: This technique artificially expands the training dataset by applying various transformations, such as rotation, scaling, or flipping, to the existing data. This increases the diversity of the training data and forces the model to learn more robust features.\n",
    "Balancing Bias and Variance:\n",
    "Regularization plays a crucial role in addressing the bias-variance tradeoff, which is fundamental to machine learning.\n",
    "By controlling the model's complexity, regularization techniques help to strike a balance between underfitting (high bias) and overfitting (high variance), leading to improved generalization performance.\n",
    "Practical Considerations:\n",
    "The choice of regularization technique and its hyperparameters can have a significant impact on the model's performance.\n",
    "Practitioners often experiment with different regularization methods and tune the hyperparameters to find the optimal balance between training set performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef154ef6-f2c0-413b-a140-ceda25fc6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on\n",
    "model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25e30f-4edf-47b5-9598-42b6dc816ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. How Dropout Works:\n",
    "\n",
    "Randomly Deactivating Neurons:\n",
    "During the training phase, Dropout randomly \"drops out\" (i.e., temporarily deactivates) a subset of the neurons in the neural network.\n",
    "This means that the outputs of the dropped-out neurons are set to zero, and they do not contribute to the forward propagation and backpropagation of the network.\n",
    "Reduced Reliance on Specific Neurons:\n",
    "By randomly dropping out neurons, Dropout forces the network to learn more robust and redundant features, rather than relying too heavily on specific neurons.\n",
    "This is because the network can no longer rely on the same set of neurons to make predictions, as the active neurons change at each training iteration.\n",
    "Ensemble Effect:\n",
    "Dropout can be seen as creating an implicit ensemble of many different \"thinned\" neural network models, where each iteration of training selects a different subset of neurons to be active.\n",
    "During inference (when the model is deployed), all neurons are used, but their outputs are scaled by the dropout probability to approximate the ensemble effect.\n",
    "Impact of Dropout on Model Training and Inference:\n",
    "\n",
    "Reduced Overfitting:\n",
    "By forcing the network to learn more robust features and preventing it from relying too heavily on specific neurons, Dropout helps reduce overfitting on the training data.\n",
    "This leads to improved generalization performance on new, unseen data.\n",
    "Faster Convergence:\n",
    "Dropout can sometimes lead to faster convergence during the training process, as the network is forced to learn more generalized features that are less sensitive to the specific training examples.\n",
    "Inference-Time Behavior:\n",
    "During inference, when the model is deployed and used to make predictions, all neurons are used, but their outputs are scaled by the dropout probability.\n",
    "This scaling helps to approximate the ensemble effect created during training, where different subsets of neurons were active at each iteration.\n",
    "Improved Robustness:\n",
    "Dropout can make the neural network more robust to noisy inputs or missing data, as the network has learned to rely on a more diverse set of features.\n",
    "Hyperparameter Tuning:\n",
    "The dropout rate (the probability of a neuron being dropped out) is a crucial hyperparameter that needs to be tuned carefully. A higher dropout rate can lead to more regularization but may also slow down convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a87dc-b35f-4bfd-9200-ce49f4a5c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Describe the concept of Early ztopping as a form of regularization. How does it help prevent overfitting\n",
    "during the training process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa5cac-f933-4d22-b2fe-070aafa8b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Early stopping is a regularization technique used in machine learning, particularly in the context of deep learning, to prevent overfitting during the training process.\n",
    "\n",
    "Concept of Early Stopping:\n",
    "\n",
    "Monitoring Validation Performance:\n",
    "During the training of a neural network, the model's performance is typically evaluated on a separate validation dataset at regular intervals.\n",
    "This validation performance is used to assess the model's generalization ability, as the training loss may continue to decrease even when the model starts to overfit.\n",
    "Stopping the Training Process:\n",
    "Early stopping works by continuously monitoring the validation performance during training.\n",
    "If the validation performance starts to deteriorate, even as the training loss continues to decrease, the training process is stopped, and the model parameters are reverted to the point where the validation performance was the best.\n",
    "Preventing Overfitting:\n",
    "By stopping the training process before the model starts to overfit the training data, early stopping helps to prevent the model from memorizing the training examples and ensures that the model learns more generalizable features.\n",
    "How Early Stopping Prevents Overfitting:\n",
    "\n",
    "Identifying Optimal Stopping Point:\n",
    "Early stopping helps identify the optimal point in the training process where the model has learned the most generalizable features and has not yet started to overfit the training data.\n",
    "This is achieved by continuously monitoring the model's performance on the validation dataset and stopping the training when the validation performance starts to degrade.\n",
    "Avoiding Overtraining:\n",
    "Without early stopping, deep neural networks have the capacity to overfit the training data, particularly as the training process progresses.\n",
    "Early stopping prevents the model from being overtrained, ensuring that the final model is not overly specialized to the training data and can generalize well to new, unseen data.\n",
    "Efficient Resource Utilization:\n",
    "Early stopping can also save computational resources and training time, as it stops the training process as soon as the model's performance on the validation set starts to deteriorate.\n",
    "This is particularly important in the context of deep learning, where training large models can be computationally intensive and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575903a-b92b-44fc-bf5d-2a3232cddd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch\n",
    "Normalization help in preventing overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0324e9-a685-4ddb-ac7f-4ec0ccb8c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. Batch Normalization (BatchNorm) is a powerful technique used in deep learning that not only helps in improving the training stability and performance of neural networks but also acts as a form of regularization, helping to prevent overfitting.\n",
    "\n",
    "Concept of Batch Normalization:\n",
    "\n",
    "Normalizing Activations:\n",
    "Batch Normalization operates by normalizing the inputs to each layer of the neural network, making the distribution of these inputs more stable and uniform.\n",
    "This is done by computing the mean and variance of the inputs in a mini-batch and then shifting and scaling the inputs accordingly.\n",
    "Reducing Internal Covariate Shift:\n",
    "One of the key challenges in training deep neural networks is the problem of internal covariate shift, where the distribution of the inputs to each layer changes during the training process, making the optimization difficult.\n",
    "Batch Normalization helps to mitigate this issue by reducing the internal covariate shift, making the training process more stable and robust.\n",
    "Role of Batch Normalization as a Regularizer:\n",
    "\n",
    "Reducing Overfitting:\n",
    "Batch Normalization acts as a regularizer by introducing additional noise and forcing the network to learn more robust and generalizable features.\n",
    "The normalization process introduces a source of noise, as the mean and variance are computed using a finite mini-batch of data, rather than the entire training set.\n",
    "This noise encourages the network to learn features that are less sensitive to the specific training examples, reducing the risk of overfitting.\n",
    "Improving Generalization:\n",
    "By normalizing the inputs to each layer, Batch Normalization helps the network focus on learning more useful features, rather than relying on the scale or the distribution of the inputs.\n",
    "This, in turn, leads to better generalization performance on new, unseen data.\n",
    "Reduced Dependence on Initialization:\n",
    "Batch Normalization can make the training process less sensitive to the initial values of the network parameters, as the normalization step helps to reduce the internal covariate shift.\n",
    "This can lead to faster convergence and better performance, especially for deep neural networks.\n",
    "Regularization Effect:\n",
    "The normalization process in Batch Normalization can be seen as a form of implicit regularization, as it encourages the network to learn more robust and generalizable features.\n",
    "This regularization effect is similar to the effect of techniques like Dropout, where the network is forced to learn more redundant features to compensate for the randomly dropped-out neurons.\n",
    "How Batch Normalization Prevents Overfitting:\n",
    "\n",
    "Stabilizing the Training Process:\n",
    "By reducing the internal covariate shift, Batch Normalization helps to stabilize the training process, making it less sensitive to the initialization of the network parameters and the choice of hyperparameters.\n",
    "This stability can lead to faster convergence and better generalization performance.\n",
    "Reducing Dependence on Specific Features:\n",
    "The normalization process in Batch Normalization encourages the network to learn features that are less sensitive to the specific training examples, reducing the risk of overfitting.\n",
    "Introducing Regularization Effect:\n",
    "The noise introduced by the finite-sample estimation of the mean and variance in Batch Normalization acts as a form of implicit regularization, forcing the network to learn more robust and generalizable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0381a061-777f-490e-a081-8214e2147abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d045c228-6fdf-4581-a4f3-757d96ad9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c199f1-cc0d-46f4-8ab2-c2a7439420a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtr,ytr),(Xte,yte)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ddff03-1b89-43cc-87e5-52bb7fd677e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr=Xtr/255\n",
    "Xte=Xte/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f9f360-4837-4da5-b3f5-9fa2bd1169f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr,Xval=Xtr[5000:],Xtr[:5000]\n",
    "ytr,yval=ytr[5000:],ytr[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e081dc86-3333-442c-87e2-96bcd1615038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten as flat, Dense as dense\n",
    "from keras.models import Sequential as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ed32f0-3749-49c3-b364-04d935896885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947d0fec-13ba-4e25-a3bb-07e32eb30b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e265973-98c8-4c5e-81f8-e86e88a68121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():        \n",
    "    model=seq()\n",
    "    model.add(flat(input_shape=[28,28]))\n",
    "    model.add(dense(300,activation='relu'))\n",
    "    model.add(dense(100,activation='relu'))\n",
    "    model.add(dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83342623-0d5f-402d-825c-208762557485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab224cd0-8f0d-4e79-b341-c43088869a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fc37d2-ddf3-4a1e-8074-ce8c67a2e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2467 - accuracy: 0.9295 - val_loss: 0.2082 - val_accuracy: 0.9442\n",
      "Epoch 2/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2092 - accuracy: 0.9409 - val_loss: 0.1814 - val_accuracy: 0.9498\n",
      "Epoch 3/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1823 - accuracy: 0.9484 - val_loss: 0.1613 - val_accuracy: 0.9544\n",
      "Epoch 4/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1602 - accuracy: 0.9540 - val_loss: 0.1462 - val_accuracy: 0.9608\n",
      "Epoch 5/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1423 - accuracy: 0.9598 - val_loss: 0.1323 - val_accuracy: 0.9644\n",
      "Epoch 6/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1278 - accuracy: 0.9641 - val_loss: 0.1220 - val_accuracy: 0.9658\n",
      "Epoch 7/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1155 - accuracy: 0.9671 - val_loss: 0.1133 - val_accuracy: 0.9686\n",
      "Epoch 8/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1049 - accuracy: 0.9709 - val_loss: 0.1107 - val_accuracy: 0.9692\n",
      "Epoch 9/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0962 - accuracy: 0.9728 - val_loss: 0.1014 - val_accuracy: 0.9718\n",
      "Epoch 10/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0885 - accuracy: 0.9753 - val_loss: 0.0982 - val_accuracy: 0.9738\n",
      "Epoch 11/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0818 - accuracy: 0.9767 - val_loss: 0.0938 - val_accuracy: 0.9748\n",
      "Epoch 12/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0753 - accuracy: 0.9790 - val_loss: 0.0899 - val_accuracy: 0.9754\n",
      "Epoch 13/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0702 - accuracy: 0.9805 - val_loss: 0.0857 - val_accuracy: 0.9760\n",
      "Epoch 14/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0652 - accuracy: 0.9821 - val_loss: 0.0834 - val_accuracy: 0.9762\n",
      "Epoch 15/15\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0801 - val_accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(Xtr,ytr,validation_data=(Xval,yval),batch_size=32,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78812346-c836-4755-aeb0-b87dd186d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ad8a7b7-6f02-4d81-9b10-ae176ee5b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0186486a-4a43-436a-89c9-618ac3a06a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(flat(input_shape=[28,28]))\n",
    "model2.add(bn())\n",
    "model2.add(dense(300,activation='relu'))\n",
    "model2.add(bn())\n",
    "model2.add(dense(100,activation='relu'))\n",
    "model2.add(bn())\n",
    "model2.add(dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "682a6d7f-874f-4a70-bbba-79ecaa049ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68fa5b27-5b7d-48be-bbe6-bc7b4ba551f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.3689 - accuracy: 0.8888 - val_loss: 0.1961 - val_accuracy: 0.9432\n",
      "Epoch 2/15\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1862 - accuracy: 0.9447 - val_loss: 0.1531 - val_accuracy: 0.9548\n",
      "Epoch 3/15\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1421 - accuracy: 0.9581 - val_loss: 0.1353 - val_accuracy: 0.9604\n",
      "Epoch 4/15\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1173 - accuracy: 0.9644 - val_loss: 0.1183 - val_accuracy: 0.9650\n",
      "Epoch 5/15\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0946 - accuracy: 0.9724 - val_loss: 0.1141 - val_accuracy: 0.9680\n",
      "Epoch 6/15\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0805 - accuracy: 0.9762 - val_loss: 0.1093 - val_accuracy: 0.9680\n",
      "Epoch 7/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0734 - accuracy: 0.9778 - val_loss: 0.1057 - val_accuracy: 0.9704\n",
      "Epoch 8/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0636 - accuracy: 0.9808 - val_loss: 0.1024 - val_accuracy: 0.9714\n",
      "Epoch 9/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0579 - accuracy: 0.9826 - val_loss: 0.1049 - val_accuracy: 0.9710\n",
      "Epoch 10/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.1025 - val_accuracy: 0.9700\n",
      "Epoch 11/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.1025 - val_accuracy: 0.9704\n",
      "Epoch 12/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0412 - accuracy: 0.9879 - val_loss: 0.0990 - val_accuracy: 0.9720\n",
      "Epoch 13/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0384 - accuracy: 0.9886 - val_loss: 0.1055 - val_accuracy: 0.9720\n",
      "Epoch 14/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0346 - accuracy: 0.9899 - val_loss: 0.1015 - val_accuracy: 0.9732\n",
      "Epoch 15/15\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0322 - accuracy: 0.9912 - val_loss: 0.1043 - val_accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "hist2=model2.fit(Xtr,ytr,validation_data=(Xval,yval),batch_size=32,epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561204f-c9c4-40ee-a11c-8e51e148e701",
   "metadata": {},
   "source": [
    "# The accuaracy of model without batch normalisation is 98.3 and with batch normalisation is 99.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0024f4-f418-4cc0-b35a-da4bed1761cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
