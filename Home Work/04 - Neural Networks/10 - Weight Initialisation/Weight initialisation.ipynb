{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4acf33-9b9b-44af-aa08-afefcc33b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize\n",
    "the weights carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d792f7f-09f5-429a-a120-25ed404485c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Convergence Speed: The initial values of the weights in an ANN can significantly impact the convergence speed of the training process. If the weights are initialized with poor values, the model may take a long time to converge or may even fail to converge at all. Proper weight initialization can help the model converge faster, leading to more efficient training.\n",
    "Avoidance of Vanishing or Exploding Gradients: During the backpropagation process, the gradients of the loss function with respect to the weights are calculated. If the weights are initialized with values that are too small or too large, it can lead to the vanishing or exploding gradient problem, respectively. This can hinder the model's ability to learn effectively and update the weights appropriately.\n",
    "Symmetry Breaking: If all the weights in an ANN are initialized to the same value, the network will suffer from a symmetry problem. This means that the hidden units will learn the same features, leading to suboptimal performance. Careful weight initialization helps to break this symmetry and encourage the hidden units to learn different features.\n",
    "Generalization Capability: The initial weights can also affect the model's ability to generalize to new, unseen data. If the weights are initialized poorly, the model may overfit to the training data, reducing its performance on the validation and test sets.\n",
    "It is necessary to initialize the weights carefully for the following reasons:\n",
    "\n",
    "Avoid Saturation of Activation Functions: If the weights are initialized with very large or very small values, the activation functions in the network (e.g., sigmoid, tanh) may become saturated, leading to slow or no learning during the training process.\n",
    "Maintain Numerical Stability: Poorly initialized weights can lead to numerical instability during the training process, such as overflow or underflow errors, which can prevent the model from converging.\n",
    "Reduce the Risk of Getting Stuck in Poor Local Minima: Careful weight initialization can help the model find better local minima during the optimization process, improving the overall performance of the ANN.\n",
    "Improve the Consistency of Training: With proper weight initialization, the training process becomes more consistent, meaning that different runs of the model with the same hyperparameters will converge to similar performance levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a14731-b867-425b-8d2d-f0aac39fdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the challenges associated with improper weight initialization. How do these issues affect model\n",
    "training and convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54028ad5-2ffe-4785-b0f4-7547bdd33540",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Vanishing or Exploding Gradients:\n",
    "If the initial weights are too small (vanishing gradients) or too large (exploding gradients), the gradients computed during backpropagation can become extremely small or large.\n",
    "This can cause the learning process to stall or diverge, as the model is unable to effectively update the weights during training.\n",
    "Vanishing gradients can lead to the lower layers of the network learning very slowly, while exploding gradients can cause the optimization process to become unstable.\n",
    "Saturation of Activation Functions:\n",
    "If the initial weights are too large, the inputs to the activation functions (e.g., sigmoid, tanh) can become very large or very small, causing the activation functions to operate in their saturated regions.\n",
    "In the saturated regions, the gradients become very small, which can significantly slow down the learning process or even prevent the model from learning at all.\n",
    "Symmetry and Redundancy:\n",
    "If all the weights are initialized to the same value, the network will suffer from a symmetry problem, where the hidden units learn the same features.\n",
    "This can lead to redundancy in the network and suboptimal performance, as the hidden units are not able to learn diverse features.\n",
    "Slow Convergence:\n",
    "Improper weight initialization can lead to a longer training time and slower convergence of the model.\n",
    "If the initial weights are not well-suited for the problem at hand, the model may take a long time to find the optimal set of weights, or it may not converge at all.\n",
    "Poor Generalization:\n",
    "Improper weight initialization can also affect the model's ability to generalize to unseen data.\n",
    "If the weights are initialized in a way that causes the model to overfit to the training data, the model's performance on the validation and test sets may be poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04e43b-549f-4b11-b1b6-dc62ea08cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Discuss the concept of variance and how it relates to weight initialization. WhE is it crucial to consider the\n",
    "variance of weights during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05904e73-4827-4fb0-856d-b1da28821ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. The concept of variance is closely related to weight initialization in artificial neural networks. Ensuring the right variance of the initialized weights is crucial for the effective training and convergence of the model.\n",
    "\n",
    "Variance refers to the spread or dispersion of the values around the mean. In the context of weight initialization, the variance of the initialized weights can have a significant impact on the performance of the neural network.\n",
    "\n",
    "Here's why considering the variance of the weights during initialization is crucial:\n",
    "\n",
    "Avoiding Vanishing or Exploding Gradients:\n",
    "If the variance of the initialized weights is too small, the inputs to the activation functions (e.g., sigmoid, tanh) will be small, leading to the vanishing gradient problem.\n",
    "Conversely, if the variance of the initialized weights is too large, the inputs to the activation functions will be large, leading to the exploding gradient problem.\n",
    "Both of these issues can significantly hinder the training process and prevent the model from converging effectively.\n",
    "Maintaining Numerical Stability:\n",
    "Extreme variances in the initialized weights can lead to numerical instability during the training process, such as overflow or underflow errors.\n",
    "This can cause the optimization algorithms to become unstable and prevent the model from converging to a stable solution.\n",
    "Promoting Efficient Learning:\n",
    "The variance of the initialized weights can affect the scale of the inputs to the neurons in the network.\n",
    "If the inputs are too small or too large, the activation functions may operate in their saturated regions, slowing down the learning process.\n",
    "Appropriate variance in the initialized weights can help ensure that the inputs to the neurons are within the optimal range for efficient learning.\n",
    "Avoiding Symmetry and Redundancy:\n",
    "If all the weights are initialized with the same value, the network will suffer from a symmetry problem, where the hidden units learn the same features.\n",
    "Proper variance in the initialized weights can help break this symmetry and encourage the hidden units to learn diverse features, leading to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecf57b-45a4-4743-82df-05bb9d946ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate\n",
    "to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e22c96-bf6d-4eb5-90cb-d5d12c9eaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909bf0db-4142-47e5-b5aa-041c2baf90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6970d65-5606-4248-bc1c-bcaedb3f6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtr,ytr),(Xte,yte)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb86977-aaf1-4ea9-b606-0ba9d4ed5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr=Xtr/255\n",
    "Xte=Xte/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523064ad-1de3-441d-be1c-1ec227a210c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr,Xval=Xtr[5000:],Xtr[:5000]\n",
    "ytr,yval=ytr[5000:],ytr[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44365cc4-07bd-4b67-9597-9ca2d3b2a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential as seq\n",
    "from keras.layers import Flatten as flat, Dense as dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3bfc6b-ea09-43c0-ad41-bc15a6da67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735398e1-dd95-44f6-9bb2-046533671a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756a0aee-4cc5-4b8d-a0b9-dee61d1d9a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fca2bc9-4339-4d83-9c2b-f44991282e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(flat(input_shape=[28,28]))\n",
    "model.add(dense(300,activation='relu'))\n",
    "model.add(dense(100,activation='relu'))\n",
    "model.add(dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b806b8-d393-4407-a33d-b44023751021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48505a54-38ec-40af-be3a-44ea9e79e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a34f9d2-6ec2-4c86-b4dc-9197e121799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6222 - accuracy: 0.8376 - val_loss: 0.3097 - val_accuracy: 0.9156\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2921 - accuracy: 0.9173 - val_loss: 0.2417 - val_accuracy: 0.9314\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2388 - accuracy: 0.9326 - val_loss: 0.2062 - val_accuracy: 0.9422\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2046 - accuracy: 0.9426 - val_loss: 0.1830 - val_accuracy: 0.9478\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1789 - accuracy: 0.9491 - val_loss: 0.1599 - val_accuracy: 0.9580\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1583 - accuracy: 0.9545 - val_loss: 0.1457 - val_accuracy: 0.9622\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1419 - accuracy: 0.9595 - val_loss: 0.1343 - val_accuracy: 0.9650\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1281 - accuracy: 0.9634 - val_loss: 0.1234 - val_accuracy: 0.9668\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1170 - accuracy: 0.9667 - val_loss: 0.1142 - val_accuracy: 0.9712\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1069 - accuracy: 0.9700 - val_loss: 0.1087 - val_accuracy: 0.9730\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0977 - accuracy: 0.9730 - val_loss: 0.1035 - val_accuracy: 0.9726\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0902 - accuracy: 0.9747 - val_loss: 0.1012 - val_accuracy: 0.9726\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0831 - accuracy: 0.9767 - val_loss: 0.0959 - val_accuracy: 0.9748\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0772 - accuracy: 0.9788 - val_loss: 0.0911 - val_accuracy: 0.9740\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0719 - accuracy: 0.9800 - val_loss: 0.0878 - val_accuracy: 0.9750\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0666 - accuracy: 0.9816 - val_loss: 0.0854 - val_accuracy: 0.9752\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0623 - accuracy: 0.9830 - val_loss: 0.0829 - val_accuracy: 0.9752\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.0817 - val_accuracy: 0.9776\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0544 - accuracy: 0.9853 - val_loss: 0.0775 - val_accuracy: 0.9786\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.0769 - val_accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(Xtr,ytr,validation_data=(Xval,yval),batch_size=32,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eda3649c-91c9-42fe-ae18-37aa3fb93980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal,zero,random_normal\n",
    "from sklearn.metrics import accuracy_score as acs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e648f9-5505-456f-b918-a12f4db2bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialisers={'he_normal':he_normal,'zero':zero,'random_normal':random_normal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f153150c-3d52-468f-9ea2-54861eab4242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5963 - accuracy: 0.8428 - val_loss: 0.2993 - val_accuracy: 0.9152\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2792 - accuracy: 0.9209 - val_loss: 0.2325 - val_accuracy: 0.9374\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2284 - accuracy: 0.9352 - val_loss: 0.2041 - val_accuracy: 0.9452\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1967 - accuracy: 0.9436 - val_loss: 0.1766 - val_accuracy: 0.9522\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1730 - accuracy: 0.9501 - val_loss: 0.1590 - val_accuracy: 0.9582\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "The accuracy of he_normal is: 0.9536\n",
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3017 - accuracy: 0.1120 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1123 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1123 - val_loss: 2.3008 - val_accuracy: 0.1126\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "The accuracy of zero is: 0.1135\n",
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6704 - accuracy: 0.8304 - val_loss: 0.3240 - val_accuracy: 0.9136\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.9127 - val_loss: 0.2531 - val_accuracy: 0.9286\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2528 - accuracy: 0.9274 - val_loss: 0.2158 - val_accuracy: 0.9404\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2174 - accuracy: 0.9381 - val_loss: 0.1894 - val_accuracy: 0.9458\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1912 - accuracy: 0.9454 - val_loss: 0.1695 - val_accuracy: 0.9524\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "The accuracy of random_normal is: 0.9483\n"
     ]
    }
   ],
   "source": [
    "for key,val in initialisers.items():\n",
    "    model=seq()\n",
    "    model.add(flat(input_shape=[28,28]))\n",
    "    model.add(dense(300,activation='relu',kernel_initializer=val))\n",
    "    model.add(dense(100,activation='relu',kernel_initializer=val))\n",
    "    model.add(dense(10,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='SGD')\n",
    "    hist=model.fit(Xtr,ytr,validation_data=(Xval,yval),batch_size=32,epochs=5)\n",
    "    y_pred=model.predict(Xte)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    print(f'\\nThe accuracy of {key} is: {acs(y_pred,yte)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b261f-fb71-4e47-92ff-c191757e570c",
   "metadata": {},
   "source": [
    "# Among the initialisers of He_normal zero and random, he_normal is the best model with 95.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a64917-5a4a-47ea-a4d6-c429fadf311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
