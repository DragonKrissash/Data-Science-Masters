{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d9838-373e-43d8-8f17-3f9f665745b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the role of optimization algorithms in artificial neural networksK Why are they necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390c23c-7e2d-4c38-a37b-b2c043ef228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. Optimization algorithms play a crucial role in the training and performance of artificial neural networks (ANNs). They are necessary for the following reasons:\n",
    "\n",
    "Parameter Optimization:\n",
    "ANNs have a large number of parameters (weights and biases) that need to be adjusted during the training process.\n",
    "Optimization algorithms, such as gradient descent, are used to iteratively update these parameters to minimize the loss function (the difference between the predicted output and the desired output) of the neural network.\n",
    "The optimization algorithm searches for the optimal set of parameters that result in the best performance of the neural network on the training data.\n",
    "Convergence and Efficiency:\n",
    "Without an effective optimization algorithm, the training process of an ANN would be inefficient and might not converge to an optimal solution.\n",
    "Optimization algorithms help the neural network converge to a set of parameters that minimizes the loss function, enabling the network to learn and generalize well on new, unseen data.\n",
    "Generalization Capability:\n",
    "Optimization algorithms play a crucial role in the generalization capability of the neural network.\n",
    "By optimizing the parameters to minimize the training loss, the optimization algorithm helps the network learn the underlying patterns in the data, rather than simply memorizing the training examples.\n",
    "This generalization capability allows the neural network to perform well on new, unseen data, which is the ultimate goal of machine learning.\n",
    "Scalability and Complexity:\n",
    "As neural networks become larger and more complex, the number of parameters to be optimized increases exponentially.\n",
    "Efficient optimization algorithms are necessary to handle the high-dimensional parameter space and enable training of large-scale neural networks.\n",
    "Advancements in optimization algorithms, such as stochastic gradient descent, momentum, and adaptive learning rates, have been crucial in making the training of deep neural networks feasible and practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9af6ed-b657-44d5-a73a-dea378a974d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms\n",
    "of convergence speed and memory requirements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa475c82-853d-4d6b-a2be-757240c678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2. Gradient Descent:\n",
    "Gradient descent is an iterative optimization algorithm that adjusts the parameters (weights and biases) of a model in the direction of the negative gradient of the loss function.\n",
    "The gradient represents the slope of the loss function with respect to the parameters, indicating the direction of the steepest descent.\n",
    "The algorithm updates the parameters by taking a step in the direction of the negative gradient, moving towards the minimum of the loss function.\n",
    "The step size, or learning rate, determines how large the update steps are, and it is a crucial hyperparameter that affects the convergence speed and stability of the algorithm.\n",
    "\n",
    "    Variants of Gradient Descent:\n",
    "    \n",
    "Batch Gradient Descent: Computes the gradient and updates the parameters using the entire training dataset at once.\n",
    "Stochastic Gradient Descent (SGD): Updates the parameters using a single training example at a time, resulting in a noisy but faster convergence.\n",
    "Mini-batch Gradient Descent: Updates the parameters using a small subset of the training data (mini-batch) at a time, balancing the convergence speed and stability between batch and stochastic gradient descent.\n",
    "\n",
    "    Tradeoffs and Differences:\n",
    "    \n",
    "Convergence Speed:\n",
    "Stochastic Gradient Descent (SGD) typically converges faster than batch gradient descent, as it updates the parameters more frequently and can escape local minima more easily.\n",
    "Mini-batch gradient descent offers a compromise, with a faster convergence speed than batch gradient descent and more stability than SGD.\n",
    "\n",
    "    Memory Requirements:\n",
    "Batch gradient descent requires storing the entire training dataset in memory, which can be a significant limitation for large datasets.\n",
    "Stochastic gradient descent only requires storing a single training example at a time, making it more memory-efficient.\n",
    "Mini-batch gradient descent requires storing a small subset of the training data (the mini-batch), striking a balance between memory requirements and convergence speed.\n",
    "\n",
    "    Noise and Stability:\n",
    "Stochastic gradient descent introduces more noise in the parameter updates due to the high variance of the gradients computed on single examples.\n",
    "Batch gradient descent has less noise but can be more susceptible to getting stuck in local minima.\n",
    "Mini-batch gradient descent balances the trade-off between the noise of SGD and the stability of batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce713ea-3efd-4951-89b7-4d24058f119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\n",
    "convergence, local minima. How do modern optimizers address these challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19631b49-f209-416a-9cf9-6a5cd218cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3. Slow Convergence:\n",
    "Vanilla gradient descent can converge slowly, especially for complex and high-dimensional optimization problems, such as training deep neural networks.\n",
    "The learning rate is a crucial hyperparameter that affects the convergence speed. If the learning rate is too small, the updates will be too small, leading to slow convergence. If the learning rate is too large, the updates may overshoot the minimum, causing the optimization to diverge.\n",
    "Local Minima:\n",
    "Gradient-based optimization methods, such as gradient descent, are prone to getting stuck in local minima, where the gradient is zero, but the solution is not the global minimum.\n",
    "Neural networks with complex loss landscapes are particularly susceptible to this challenge, as the loss function can have many local minima, making it difficult to find the global minimum.\n",
    "Vanishing or Exploding Gradients:\n",
    "In deep neural networks, the gradients can either vanish (become extremely small) or explode (become extremely large) as they propagate back through the network during backpropagation.\n",
    "This can lead to poor parameter updates, causing the optimization to stall or diverge.\n",
    "Generalization and Overfitting:\n",
    "Vanilla gradient descent can sometimes lead to overfitting, where the model performs well on the training data but fails to generalize to new, unseen data.\n",
    "This is because the optimization may focus too much on minimizing the training loss, without adequately considering the model's ability to generalize.\n",
    "To address these challenges, modern optimization algorithms have been developed, including:\n",
    "\n",
    "Adaptive Optimization Algorithms:\n",
    "Algorithms like AdaGrad, RMSProp, and Adam (Adaptive Moment Estimation) adapt the learning rate for each parameter individually based on the historical gradients.\n",
    "This helps mitigate the issue of slow convergence, as the learning rate can be adjusted dynamically for different parameters.\n",
    "Momentum-based Optimizers:\n",
    "Momentum-based methods, such as the Nesterov Accelerated Gradient and the traditional momentum, incorporate a moving average of past gradients into the update rule.\n",
    "This helps the optimization algorithm build up momentum, allowing it to escape from narrow valleys and local minima more effectively.\n",
    "Second-Order Optimization Methods:\n",
    "Algorithms like Newton's method and the Hessian-free optimization consider the curvature of the loss function, not just the gradients.\n",
    "By incorporating second-order information, these methods can converge more quickly and handle the challenges of vanishing or exploding gradients better.\n",
    "Regularization Techniques:\n",
    "Methods like L1/L2 regularization, dropout, and batch normalization can help prevent overfitting and improve the generalization performance of the model.\n",
    "These techniques regularize the optimization process, encouraging the model to learn more robust features and representations.\n",
    "Gradient Clipping:\n",
    "Gradient clipping is a technique that limits the magnitude of the gradients, preventing the exploding gradients problem in deep neural networks.\n",
    "By capping the gradients, the optimization process becomes more stable and less prone to divergence.\n",
    "Curriculum Learning and Warm-Up:\n",
    "These techniques start the optimization with simpler tasks or lower learning rates and gradually increase the complexity or learning rate over time.\n",
    "This can help the optimization process avoid getting stuck in poor local minima and improve the final performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608d845-fae3-49b7-8321-f5dcaf433178",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do\n",
    "they impact convergence and model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198a2e0-7fe7-4236-973e-871031ef83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4. Momentum and learning rate are two important concepts in the context of optimization algorithms for training neural networks. Let's discuss how they impact convergence and model performance:\n",
    "\n",
    "Momentum:\n",
    "Momentum is a technique that helps accelerate the convergence of gradient-based optimization algorithms, such as stochastic gradient descent (SGD).\n",
    "The key idea behind momentum is to incorporate a term that accumulates a running average of the past gradients, essentially building up \"momentum\" in the direction of the negative gradient.\n",
    "Mathematically, the momentum update rule is:\n",
    "v_t = β * v_t-1 + (1 - β) * ∇L(θ_t-1)\n",
    "θ_t = θ_t-1 - η * v_t\n",
    "Where v_t is the velocity (momentum term), β is the momentum coefficient (typically between 0.9 and 0.999), ∇L(θ_t-1) is the gradient, and η is the learning rate.\n",
    "Momentum helps the optimization algorithm gain \"inertia\" and move faster in the direction of the negative gradient, especially in scenarios where the gradients are noisy or the loss landscape has narrow valleys.\n",
    "This results in faster convergence and helps the optimization escape from saddle points or shallow local minima more effectively.\n",
    "Learning Rate:\n",
    "The learning rate (η) is a crucial hyperparameter that determines the step size of the updates in the optimization algorithm.\n",
    "A larger learning rate can lead to faster convergence, but if set too high, it can cause the optimization to diverge and become unstable.\n",
    "A smaller learning rate, on the other hand, can result in slower convergence, as the updates will be smaller and the optimization will take more iterations to reach the minimum.\n",
    "The choice of the learning rate is a delicate balance, as it depends on the complexity of the problem, the scale of the gradients, and the specific optimization algorithm being used.\n",
    "Adaptive learning rate algorithms, such as AdaGrad, RMSProp, and Adam, automatically adjust the learning rate for each parameter based on the historical gradients, which can improve the overall convergence and stability of the optimization process.\n",
    "Impact on Convergence and Model Performance:\n",
    "\n",
    "Momentum:\n",
    "Momentum generally improves the convergence speed of the optimization algorithm, as it helps the updates gain \"inertia\" and move more effectively towards the minimum.\n",
    "This can lead to faster training times and, in many cases, better final model performance, as the optimization is able to escape from local minima more effectively.\n",
    "Learning Rate:\n",
    "The learning rate has a significant impact on the convergence and performance of the model.\n",
    "A well-chosen learning rate can lead to faster convergence and better final model performance, while a suboptimal learning rate can cause the optimization to diverge or get stuck in poor local minima.\n",
    "Adaptive learning rate algorithms, such as Adam, can help mitigate the challenges of setting a suitable learning rate by automatically adjusting it during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcbd2e-3065-4644-9dad-5758722a7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Explain the concept of Stochastic radient Descent (SGD) and its advantages compared to traditional\n",
    "gradient descent. Discuss its limitations and scenarios where it is most suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a340391-39e1-4159-bbff-58887362efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5. Stochastic Gradient Descent (SGD) is an optimization algorithm that is widely used in the training of neural networks and other machine learning models. It is a variant of the traditional gradient descent algorithm and has several advantages compared to its predecessor.\n",
    "\n",
    "Concept of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "In traditional gradient descent, the gradient of the loss function is calculated using the entire training dataset, and the parameters are updated accordingly.\n",
    "In contrast, SGD calculates the gradient using a single training example (or a small subset of the training data, known as a mini-batch) and updates the parameters based on this gradient.\n",
    "The key difference is that the gradient computed using a single example (or mini-batch) is a noisy estimate of the true gradient, but this noise can actually be beneficial for the optimization process.\n",
    "Advantages of SGD:\n",
    "\n",
    "Faster Convergence:\n",
    "SGD can converge much faster than traditional gradient descent, especially for large-scale datasets, as it updates the parameters more frequently.\n",
    "The noisy gradient updates can help the optimization escape local minima and saddle points more easily.\n",
    "Handling of Large Datasets:\n",
    "SGD can efficiently handle large training datasets, as it only requires storing a single example (or mini-batch) in memory at a time, unlike traditional gradient descent, which needs to store the entire dataset.\n",
    "This makes SGD more memory-efficient and scalable to large-scale problems.\n",
    "Online and Incremental Learning:\n",
    "SGD can be used for online and incremental learning, where the model is updated as new data becomes available, without the need to re-process the entire dataset.\n",
    "This is particularly useful in scenarios where the data is continuously generated or the problem domain is constantly evolving.\n",
    "Limitations and Scenarios for SGD:\n",
    "\n",
    "Noisy Gradients:\n",
    "The stochastic nature of SGD can lead to noisy gradients, which can cause the optimization to oscillate around the minimum, potentially slowing down the convergence.\n",
    "This can be more pronounced in the later stages of the optimization, when the model is closer to the minimum.\n",
    "Hyperparameter Tuning:\n",
    "SGD is more sensitive to the choice of hyperparameters, such as the learning rate, compared to traditional gradient descent.\n",
    "Selecting the appropriate learning rate (and potentially other hyperparameters, such as the mini-batch size) can be more challenging and requires more experimentation.\n",
    "Batch Normalization and Regularization:\n",
    "Certain techniques, such as batch normalization and some regularization methods, work better with traditional gradient descent, as they rely on the entire dataset's statistics.\n",
    "Adapting these techniques for SGD can be more complex and may require additional modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ecac7-f619-4886-bd3f-85558d3c7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates.\n",
    "Discuss its benefits and potential drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692add2b-17f1-4b04-a6c4-819c47850bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6. Adam (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of two popular techniques: momentum and adaptive learning rates. Adam is widely used in the training of deep neural networks and other machine learning models.\n",
    "\n",
    "Concept of Adam Optimizer:\n",
    "\n",
    "Momentum:\n",
    "Adam incorporates the concept of momentum, similar to the momentum-based optimization algorithms like Nesterov Accelerated Gradient and RMSProp.\n",
    "Momentum helps accelerate the updates in the direction of the negative gradient, similar to a ball rolling down a hill, gaining speed and momentum.\n",
    "Adaptive Learning Rates:\n",
    "Adam also adapts the learning rate for each parameter individually, similar to the AdaGrad and RMSProp algorithms.\n",
    "It maintains an exponentially decaying average of the squared gradients and uses this information to adjust the learning rate for each parameter.\n",
    "The key steps in the Adam update rule are as follows:\n",
    "\n",
    "Compute the gradient of the loss function with respect to the parameters.\n",
    "Compute the exponentially decaying average of the past gradients (the first moment, or the mean) and the exponentially decaying average of the past squared gradients (the second moment, or the uncentered variance).\n",
    "Use the first and second moments to compute an update to the parameters that combines the benefits of momentum and adaptive learning rates.\n",
    "Benefits of Adam Optimizer:\n",
    "\n",
    "Faster Convergence:\n",
    "The combination of momentum and adaptive learning rates allows Adam to converge faster than traditional gradient descent and other optimization algorithms, especially for non-convex optimization problems.\n",
    "Robustness to Noisy Gradients:\n",
    "Adam is relatively robust to noisy gradients, which can be common in the training of deep neural networks.\n",
    "The adaptive learning rates help to adapt the updates for each parameter, mitigating the impact of noisy gradients.\n",
    "Automatic Tuning of Learning Rates:\n",
    "Adam eliminates the need to manually tune the learning rate, as it automatically adjusts the learning rate for each parameter based on the first and second moments of the gradients.\n",
    "This can significantly reduce the time and effort required for hyperparameter tuning.\n",
    "Potential Drawbacks of Adam Optimizer:\n",
    "\n",
    "Memory Requirements:\n",
    "Adam requires maintaining the first and second moments of the gradients, which increases the memory requirements compared to simpler optimization algorithms like SGD.\n",
    "This can be a concern for training very large models on limited hardware resources.\n",
    "Sensitivity to Hyperparameters:\n",
    "While Adam is less sensitive to the learning rate compared to traditional gradient descent, it does have other hyperparameters, such as the decay rates for the first and second moments, that need to be tuned.\n",
    "Suboptimal hyperparameter settings can negatively impact the performance of the optimizer.\n",
    "Potential for Generalization Issues:\n",
    "In some cases, Adam has been observed to perform worse than simpler optimizers, such as SGD with momentum, in terms of the model's generalization performance.\n",
    "This is an active area of research, and the reasons behind this behavior are not yet fully understood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb66a5d-ce88-4a56-ad52-194533b4272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning\n",
    "rates. Compare it with Adam and discuss their relative strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95f268-732b-46f4-b0c5-61be0c814776",
   "metadata": {},
   "outputs": [],
   "source": [
    "A7. RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm that was developed to address some of the challenges faced by earlier adaptive learning rate methods like AdaGrad.\n",
    "\n",
    "Concept of RMSprop Optimizer:\n",
    "\n",
    "Adaptive Learning Rates:\n",
    "Like AdaGrad, RMSprop maintains a per-parameter adaptive learning rate that is scaled inversely proportional to the square root of the sum of the historical squared gradients.\n",
    "This helps the optimizer adapt the learning rate for each parameter based on the magnitude of the gradients, allowing it to make larger updates for parameters with small gradients and smaller updates for parameters with large gradients.\n",
    "Exponentially Weighted Moving Average:\n",
    "RMSprop uses an exponentially weighted moving average (EWMA) of the squared gradients, instead of the cumulative sum used in AdaGrad.\n",
    "This allows RMSprop to adapt to the local geometry of the loss function more effectively, especially in the later stages of training.\n",
    "The RMSprop update rule is as follows:\n",
    "\n",
    "e_t = γ * e_t-1 + (1 - γ) * (∇L(θ_t-1))^2\n",
    "θ_t = θ_t-1 - η / sqrt(e_t + ε) * ∇L(θ_t-1)\n",
    "Where e_t is the exponentially weighted moving average of the squared gradients, γ is the decay rate (typically around 0.9), η is the learning rate, and ε is a small constant to prevent division by zero.\n",
    "Comparison with Adam:\n",
    "\n",
    "Adaptive Learning Rates:\n",
    "Both RMSprop and Adam use adaptive learning rates, but they differ in how they compute and update the learning rates.\n",
    "RMSprop uses an EWMA of the squared gradients, while Adam maintains both the first and second moments (mean and uncentered variance) of the gradients.\n",
    "Momentum:\n",
    "Adam incorporates momentum, which can help accelerate the convergence, especially in the presence of noisy gradients.\n",
    "RMSprop does not have a built-in momentum term, but it can be combined with a separate momentum update.\n",
    "Strengths and Weaknesses:\n",
    "Strengths of RMSprop:\n",
    "\n",
    "Simpler implementation and fewer hyperparameters compared to Adam.\n",
    "Effective in addressing the diminishing learning rate problem of AdaGrad.\n",
    "Performs well on a wide range of problems, especially when combined with momentum.\n",
    "Strengths of Adam:\n",
    "\n",
    "Incorporates both adaptive learning rates and momentum, which can lead to faster convergence in many cases.\n",
    "Automatic tuning of hyperparameters, reducing the need for manual tuning.\n",
    "Robust to noisy gradients, making it suitable for training deep neural networks.\n",
    "Weaknesses of RMSprop:\n",
    "\n",
    "Does not have a built-in momentum term, which can slow down convergence in some cases.\n",
    "The choice of the decay rate (γ) can be important and may require tuning.\n",
    "Weaknesses of Adam:\n",
    "\n",
    "Higher memory requirements due to the need to store the first and second moments of the gradients.\n",
    "Potential for generalization issues in some cases, compared to simpler optimizers like SGD with momentum.\n",
    "Sensitivity to the choice of hyperparameters, such as the decay rates for the first and second moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6729ca4c-3f54-4c4e-9b86-5e682cad2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5117517-c8d6-40ee-b833-1681286f4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658eb40c-bd42-465d-8ddb-c18fa073bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xtr,ytr),(Xte,yte)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dad1386-85a1-421d-bc56-9ba15cf5592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr=Xtr/255\n",
    "Xte=Xte/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7a24f8-d968-4686-8657-263b7d9e65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr,Xval=Xtr[5000:],Xtr[:5000]\n",
    "ytr,yval=ytr[5000:],ytr[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a697fb-3628-4a19-b967-e8cbc242d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten as flat, Dense as dense\n",
    "from keras.models import Sequential as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566430b9-e5f3-4b83-9450-312d943a2ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873f4803-00d8-4c61-9183-76056dda6aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87971079-b4e4-4c3f-b608-fb3d0f44bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():        \n",
    "    model=seq()\n",
    "    model.add(flat(input_shape=[28,28]))\n",
    "    model.add(dense(300,activation='relu'))\n",
    "    model.add(dense(100,activation='relu'))\n",
    "    model.add(dense(10,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea7e47f-5a43-4cc5-abbf-b9183a0ac700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b037b78-5451-4a1e-baef-3d6455481222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import accuracy_score as acs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f98807de-f5dc-44cb-94ea-daa1ccd7610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f0992ab-7955-4e8d-b849-96407ce221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(y_pred,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab9d9ff7-2831-4ff4-8d32-a8799c8bd6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53c203cd-e47a-4c93-93d2-eb4cdf0f96d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6083 - accuracy: 0.8444 - val_loss: 0.3075 - val_accuracy: 0.9140\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2919 - accuracy: 0.9165 - val_loss: 0.2492 - val_accuracy: 0.9320\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2401 - accuracy: 0.9314 - val_loss: 0.2113 - val_accuracy: 0.9410\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2053 - accuracy: 0.9409 - val_loss: 0.1846 - val_accuracy: 0.9480\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1791 - accuracy: 0.9483 - val_loss: 0.1633 - val_accuracy: 0.9538\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "SGD optimiser has accuracy: 0.9504\n",
      "\n",
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2128 - accuracy: 0.9373 - val_loss: 0.1041 - val_accuracy: 0.9678\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0885 - accuracy: 0.9731 - val_loss: 0.0824 - val_accuracy: 0.9746\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.0842 - val_accuracy: 0.9752\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0432 - accuracy: 0.9856 - val_loss: 0.0847 - val_accuracy: 0.9756\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0349 - accuracy: 0.9883 - val_loss: 0.0838 - val_accuracy: 0.9776\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "adam optimiser has accuracy: 0.9813\n",
      "\n",
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2131 - accuracy: 0.9357 - val_loss: 0.1145 - val_accuracy: 0.9654\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0915 - accuracy: 0.9729 - val_loss: 0.0838 - val_accuracy: 0.9768\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0664 - accuracy: 0.9807 - val_loss: 0.0833 - val_accuracy: 0.9782\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.0812 - val_accuracy: 0.9766\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0420 - accuracy: 0.9880 - val_loss: 0.0946 - val_accuracy: 0.9774\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "\n",
      "rmsprop optimiser has accuracy: 0.9784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for optm in ['SGD','adam','rmsprop']:\n",
    "    model=make_model()\n",
    "    model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer=optm)\n",
    "    hist=model.fit(Xtr,ytr,validation_data=(Xval,yval),batch_size=32,epochs=5)\n",
    "    y_pred=model.predict(Xte)\n",
    "    y_pred=np.argmax(y_pred,axis=-1)\n",
    "    score=acs(y_pred,yte)\n",
    "    print(f'\\n{optm} optimiser has accuracy: {score}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfbd5b-34e2-4e27-8f0e-df2bd09d6c14",
   "metadata": {},
   "source": [
    "# Adam optimiser has the best accuracy among the 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623544d-3be3-4543-b4b7-4e0fbf3b93e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
