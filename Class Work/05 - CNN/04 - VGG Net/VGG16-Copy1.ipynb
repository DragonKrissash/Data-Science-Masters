{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eq3uoSrTN7r",
    "outputId": "c827801c-1f78-4672-b976-16b40efc8479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.25.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAcazjWPTVh_",
    "outputId": "562370e6-a62b-4309-e4b3-22dad24c5242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing\n",
      "  Cloning https://github.com/MihaMarkic/tflearn.git (to revision fix/is_sequence_missing) to /tmp/pip-req-build-wx9nh7pd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MihaMarkic/tflearn.git /tmp/pip-req-build-wx9nh7pd\n",
      "  Running command git checkout -b fix/is_sequence_missing --track origin/fix/is_sequence_missing\n",
      "  Switched to a new branch 'fix/is_sequence_missing'\n",
      "  Branch 'fix/is_sequence_missing' set up to track remote branch 'fix/is_sequence_missing' from 'origin'.\n",
      "  Resolved https://github.com/MihaMarkic/tflearn.git to commit 6472b8588e758ff4a33a2764d4ee638bbd0e42f0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.25.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.util.nest import is_sequence_or_composite\n",
    "!pip install git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "M2EXs2YHTXpZ",
    "outputId": "76623b08-2adb-4614-ee45-da6b9b40d769"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d7ed441cee1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLcFwWMdUhUr",
    "outputId": "38e5a503-44e5-4ef7-c2ec-a8abac506d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tflearn 0.5.0\n",
      "Uninstalling tflearn-0.5.0:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/tflearn-0.5.0.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/tflearn/*\n",
      "Proceed (Y/n)? Y\n",
      "  Successfully uninstalled tflearn-0.5.0\n",
      "Collecting tflearn\n",
      "  Using cached tflearn-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.25.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (9.4.0)\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tflearn\n",
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kMrHtrzPUvIF",
    "outputId": "518dd610-21d5-4cec-ede7-f6627212e454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.8.0\n",
      "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.9.0)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.25.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.62.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
      "Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.2\n",
      "    Uninstalling tensorboard-2.15.2:\n",
      "      Successfully uninstalled tensorboard-2.15.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "15b885c61d34434dabed366c5b0a0d48",
       "pip_warning": {
        "packages": [
         "keras",
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUrTaVDiUwUe",
    "outputId": "61f4ba57-fcbf-4f71-e255-7234fc4a8f50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xcej1RwRVI3d"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PI59iDlQXkJb"
   },
   "outputs": [],
   "source": [
    "import keras,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S__jh4-oXlTd"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lPQYCKeNY43B"
   },
   "outputs": [],
   "source": [
    "import tflearn.datasets.oxflower17 as oxflower17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pRyOG_76ZReF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGk4viYhaXuL",
    "outputId": "0c50d7af-f965-4f39-f3fd-608b7cc8e97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 60276736 / 60270631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully downloaded 17flowers.tgz 60270631 bytes.\n",
      "File Extracted\n",
      "Starting to parse images...\n",
      "Parsing Done!\n"
     ]
    }
   ],
   "source": [
    "X,y=oxflower17.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aS2LaV-RauKU"
   },
   "outputs": [],
   "source": [
    "Xtr=X.astype('float32')/255\n",
    "ytr=to_categorical(y,num_classes=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThV7siQUa_h3",
    "outputId": "1ed47b56-afcb-4185-cdf3-b1a238301c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcdpRtVsbPur",
    "outputId": "8f2ed804-224f-43ff-e7e9-62300a40f2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 17)\n"
     ]
    }
   ],
   "source": [
    "print(ytr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qmY3a5L1bSST"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dense(4096,activation='relu'))\n",
    "model.add(Dense(17,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oceMV1l3db-2",
    "outputId": "30ade0a5-c875-495f-a978-c75ae1952eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              102764544 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 17)                69649     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,330,193\n",
      "Trainable params: 134,330,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G2MfWXMkddZd"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJ82sTpqfNvo",
    "outputId": "0a07b711-03a5-481c-8859-86f31ca4a3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1088 samples, validate on 272 samples\n",
      "Epoch 1/5\n",
      "  64/1088 [>.............................] - ETA: 30:02 - loss: 2.8332 - acc: 0.0625"
     ]
    }
   ],
   "source": [
    "model.fit(Xtr,ytr,batch_size=64,epochs=5,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "aeFPSvS_f004",
    "outputId": "38b1beba-f85d-48d6-f077-1aff1e5a7e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?/export=download&id=12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n",
      "To: /content/catdog.zip\n",
      "100%|██████████| 9.09M/9.09M [00:00<00:00, 22.8MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'catdog.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "url = \"https://drive.google.com/file/d/12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP/view?usp=sharing\"\n",
    "file_id = url.split(\"/\")[-2]\n",
    "print(file_id)\n",
    "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "gdown.download(prefix+file_id, \"catdog.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7USGDaagnxx",
    "outputId": "66aed9cc-e83f-4e26-de1f-8a9476e12d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  catdog.zip\n",
      "   creating: train/\n",
      "   creating: train/Cat/\n",
      "  inflating: train/Cat/0.jpg         \n",
      "  inflating: train/Cat/1.jpg         \n",
      "  inflating: train/Cat/2.jpg         \n",
      "  inflating: train/Cat/cat.2405.jpg  \n",
      "  inflating: train/Cat/cat.2406.jpg  \n",
      "  inflating: train/Cat/cat.2436.jpg  \n",
      "  inflating: train/Cat/cat.2437.jpg  \n",
      "  inflating: train/Cat/cat.2438.jpg  \n",
      "  inflating: train/Cat/cat.2439.jpg  \n",
      "  inflating: train/Cat/cat.2440.jpg  \n",
      "  inflating: train/Cat/cat.2441.jpg  \n",
      "  inflating: train/Cat/cat.2442.jpg  \n",
      "  inflating: train/Cat/cat.2443.jpg  \n",
      "  inflating: train/Cat/cat.2444.jpg  \n",
      "  inflating: train/Cat/cat.2445.jpg  \n",
      "  inflating: train/Cat/cat.2446.jpg  \n",
      "  inflating: train/Cat/cat.2447.jpg  \n",
      "  inflating: train/Cat/cat.2448.jpg  \n",
      "  inflating: train/Cat/cat.2449.jpg  \n",
      "  inflating: train/Cat/cat.2450.jpg  \n",
      "  inflating: train/Cat/cat.2451.jpg  \n",
      "  inflating: train/Cat/cat.2452.jpg  \n",
      "  inflating: train/Cat/cat.2453.jpg  \n",
      "  inflating: train/Cat/cat.2454.jpg  \n",
      "  inflating: train/Cat/cat.2455.jpg  \n",
      "  inflating: train/Cat/cat.2456.jpg  \n",
      "  inflating: train/Cat/cat.2457.jpg  \n",
      "  inflating: train/Cat/cat.2458.jpg  \n",
      "  inflating: train/Cat/cat.2459.jpg  \n",
      "  inflating: train/Cat/cat.2460.jpg  \n",
      "  inflating: train/Cat/cat.2461.jpg  \n",
      "  inflating: train/Cat/cat.2462.jpg  \n",
      "  inflating: train/Cat/cat.2463.jpg  \n",
      "  inflating: train/Cat/cat.2464.jpg  \n",
      "  inflating: train/Cat/cat.855.jpg   \n",
      "  inflating: train/Cat/cat.856.jpg   \n",
      "  inflating: train/Cat/cat.857.jpg   \n",
      "  inflating: train/Cat/cat.858.jpg   \n",
      "  inflating: train/Cat/cat.859.jpg   \n",
      "  inflating: train/Cat/cat.86.jpg    \n",
      "  inflating: train/Cat/cat.860.jpg   \n",
      "  inflating: train/Cat/cat.861.jpg   \n",
      "  inflating: train/Cat/cat.862.jpg   \n",
      "  inflating: train/Cat/cat.863.jpg   \n",
      "  inflating: train/Cat/cat.864.jpg   \n",
      "  inflating: train/Cat/cat.865.jpg   \n",
      "  inflating: train/Cat/cat.866.jpg   \n",
      "  inflating: train/Cat/cat.867.jpg   \n",
      "  inflating: train/Cat/cat.868.jpg   \n",
      "  inflating: train/Cat/cat.869.jpg   \n",
      "  inflating: train/Cat/cat.87.jpg    \n",
      "  inflating: train/Cat/cat.870.jpg   \n",
      "  inflating: train/Cat/cat.871.jpg   \n",
      "  inflating: train/Cat/cat.872.jpg   \n",
      "  inflating: train/Cat/cat.873.jpg   \n",
      "  inflating: train/Cat/cat.874.jpg   \n",
      "  inflating: train/Cat/cat.875.jpg   \n",
      "  inflating: train/Cat/cat.876.jpg   \n",
      "  inflating: train/Cat/cat.877.jpg   \n",
      "  inflating: train/Cat/cat.878.jpg   \n",
      "  inflating: train/Cat/cat.879.jpg   \n",
      "  inflating: train/Cat/cat.88.jpg    \n",
      "  inflating: train/Cat/cat.880.jpg   \n",
      "  inflating: train/Cat/cat.881.jpg   \n",
      "  inflating: train/Cat/cat.882.jpg   \n",
      "  inflating: train/Cat/cat.883.jpg   \n",
      "  inflating: train/Cat/cat.884.jpg   \n",
      "  inflating: train/Cat/cat.885.jpg   \n",
      "  inflating: train/Cat/cat.886.jpg   \n",
      "  inflating: train/Cat/cat.887.jpg   \n",
      "  inflating: train/Cat/cat.888.jpg   \n",
      "  inflating: train/Cat/cat.889.jpg   \n",
      "  inflating: train/Cat/cat.89.jpg    \n",
      "  inflating: train/Cat/cat.890.jpg   \n",
      "  inflating: train/Cat/cat.891.jpg   \n",
      "  inflating: train/Cat/cat.892.jpg   \n",
      "  inflating: train/Cat/cat.893.jpg   \n",
      "  inflating: train/Cat/cat.894.jpg   \n",
      "  inflating: train/Cat/cat.895.jpg   \n",
      "  inflating: train/Cat/cat.896.jpg   \n",
      "  inflating: train/Cat/cat.897.jpg   \n",
      "  inflating: train/Cat/cat.898.jpg   \n",
      "  inflating: train/Cat/cat.899.jpg   \n",
      "  inflating: train/Cat/cat.9.jpg     \n",
      "  inflating: train/Cat/cat.90.jpg    \n",
      "  inflating: train/Cat/cat.900.jpg   \n",
      "  inflating: train/Cat/cat.901.jpg   \n",
      "  inflating: train/Cat/cat.902.jpg   \n",
      "  inflating: train/Cat/cat.903.jpg   \n",
      "  inflating: train/Cat/cat.904.jpg   \n",
      "  inflating: train/Cat/cat.905.jpg   \n",
      "  inflating: train/Cat/cat.906.jpg   \n",
      "  inflating: train/Cat/cat.907.jpg   \n",
      "  inflating: train/Cat/cat.908.jpg   \n",
      "  inflating: train/Cat/cat.909.jpg   \n",
      "  inflating: train/Cat/cat.91.jpg    \n",
      "  inflating: train/Cat/cat.910.jpg   \n",
      "  inflating: train/Cat/cat.911.jpg   \n",
      "  inflating: train/Cat/cat.912.jpg   \n",
      "  inflating: train/Cat/cat.913.jpg   \n",
      "  inflating: train/Cat/cat.914.jpg   \n",
      "  inflating: train/Cat/cat.915.jpg   \n",
      "  inflating: train/Cat/cat.916.jpg   \n",
      "  inflating: train/Cat/cat.917.jpg   \n",
      "  inflating: train/Cat/cat.918.jpg   \n",
      "  inflating: train/Cat/cat.919.jpg   \n",
      "  inflating: train/Cat/cat.92.jpg    \n",
      "  inflating: train/Cat/cat.920.jpg   \n",
      "  inflating: train/Cat/cat.93.jpg    \n",
      "  inflating: train/Cat/cat.94.jpg    \n",
      "  inflating: train/Cat/cat.946.jpg   \n",
      "  inflating: train/Cat/cat.947.jpg   \n",
      "  inflating: train/Cat/cat.948.jpg   \n",
      "  inflating: train/Cat/cat.949.jpg   \n",
      "  inflating: train/Cat/cat.95.jpg    \n",
      "  inflating: train/Cat/cat.950.jpg   \n",
      "  inflating: train/Cat/cat.951.jpg   \n",
      "  inflating: train/Cat/cat.952.jpg   \n",
      "  inflating: train/Cat/cat.953.jpg   \n",
      "  inflating: train/Cat/cat.954.jpg   \n",
      "  inflating: train/Cat/cat.955.jpg   \n",
      "  inflating: train/Cat/cat.956.jpg   \n",
      "  inflating: train/Cat/cat.957.jpg   \n",
      "  inflating: train/Cat/cat.958.jpg   \n",
      "  inflating: train/Cat/cat.959.jpg   \n",
      "  inflating: train/Cat/cat.96.jpg    \n",
      "  inflating: train/Cat/cat.960.jpg   \n",
      "  inflating: train/Cat/cat.961.jpg   \n",
      "  inflating: train/Cat/cat.962.jpg   \n",
      "  inflating: train/Cat/cat.963.jpg   \n",
      "  inflating: train/Cat/cat.964.jpg   \n",
      "  inflating: train/Cat/cat.965.jpg   \n",
      "  inflating: train/Cat/cat.966.jpg   \n",
      "  inflating: train/Cat/cat.967.jpg   \n",
      "  inflating: train/Cat/cat.968.jpg   \n",
      "  inflating: train/Cat/cat.969.jpg   \n",
      "  inflating: train/Cat/cat.97.jpg    \n",
      "  inflating: train/Cat/cat.970.jpg   \n",
      "  inflating: train/Cat/cat.971.jpg   \n",
      "  inflating: train/Cat/cat.972.jpg   \n",
      "  inflating: train/Cat/cat.973.jpg   \n",
      "  inflating: train/Cat/cat.974.jpg   \n",
      "  inflating: train/Cat/cat.975.jpg   \n",
      "  inflating: train/Cat/cat.976.jpg   \n",
      "  inflating: train/Cat/cat.977.jpg   \n",
      "  inflating: train/Cat/cat.978.jpg   \n",
      "  inflating: train/Cat/cat.979.jpg   \n",
      "  inflating: train/Cat/cat.98.jpg    \n",
      "  inflating: train/Cat/cat.980.jpg   \n",
      "  inflating: train/Cat/cat.981.jpg   \n",
      "  inflating: train/Cat/cat.982.jpg   \n",
      "  inflating: train/Cat/cat.983.jpg   \n",
      "  inflating: train/Cat/cat.984.jpg   \n",
      "  inflating: train/Cat/cat.985.jpg   \n",
      "  inflating: train/Cat/cat.986.jpg   \n",
      "  inflating: train/Cat/cat.987.jpg   \n",
      "  inflating: train/Cat/cat.988.jpg   \n",
      "  inflating: train/Cat/cat.989.jpg   \n",
      "  inflating: train/Cat/cat.99.jpg    \n",
      "  inflating: train/Cat/cat.990.jpg   \n",
      "  inflating: train/Cat/cat.991.jpg   \n",
      "  inflating: train/Cat/cat.992.jpg   \n",
      "  inflating: train/Cat/cat.993.jpg   \n",
      "  inflating: train/Cat/cat.994.jpg   \n",
      "  inflating: train/Cat/cat.995.jpg   \n",
      "  inflating: train/Cat/cat.996.jpg   \n",
      "  inflating: train/Cat/cat.997.jpg   \n",
      "  inflating: train/Cat/cat.998.jpg   \n",
      "  inflating: train/Cat/cat.999.jpg   \n",
      "   creating: train/Dog/\n",
      "  inflating: train/Dog/10493.jpg     \n",
      "  inflating: train/Dog/11785.jpg     \n",
      "  inflating: train/Dog/9839.jpg      \n",
      "  inflating: train/Dog/dog.2432.jpg  \n",
      "  inflating: train/Dog/dog.2433.jpg  \n",
      "  inflating: train/Dog/dog.2434.jpg  \n",
      "  inflating: train/Dog/dog.2435.jpg  \n",
      "  inflating: train/Dog/dog.2436.jpg  \n",
      "  inflating: train/Dog/dog.2437.jpg  \n",
      "  inflating: train/Dog/dog.2438.jpg  \n",
      "  inflating: train/Dog/dog.2439.jpg  \n",
      "  inflating: train/Dog/dog.2440.jpg  \n",
      "  inflating: train/Dog/dog.2441.jpg  \n",
      "  inflating: train/Dog/dog.2442.jpg  \n",
      "  inflating: train/Dog/dog.2443.jpg  \n",
      "  inflating: train/Dog/dog.2444.jpg  \n",
      "  inflating: train/Dog/dog.2445.jpg  \n",
      "  inflating: train/Dog/dog.2446.jpg  \n",
      "  inflating: train/Dog/dog.2447.jpg  \n",
      "  inflating: train/Dog/dog.2448.jpg  \n",
      "  inflating: train/Dog/dog.2449.jpg  \n",
      "  inflating: train/Dog/dog.2450.jpg  \n",
      "  inflating: train/Dog/dog.2451.jpg  \n",
      "  inflating: train/Dog/dog.2452.jpg  \n",
      "  inflating: train/Dog/dog.2453.jpg  \n",
      "  inflating: train/Dog/dog.2454.jpg  \n",
      "  inflating: train/Dog/dog.2455.jpg  \n",
      "  inflating: train/Dog/dog.2456.jpg  \n",
      "  inflating: train/Dog/dog.2457.jpg  \n",
      "  inflating: train/Dog/dog.2458.jpg  \n",
      "  inflating: train/Dog/dog.2459.jpg  \n",
      "  inflating: train/Dog/dog.2460.jpg  \n",
      "  inflating: train/Dog/dog.2461.jpg  \n",
      "  inflating: train/Dog/dog.844.jpg   \n",
      "  inflating: train/Dog/dog.845.jpg   \n",
      "  inflating: train/Dog/dog.846.jpg   \n",
      "  inflating: train/Dog/dog.847.jpg   \n",
      "  inflating: train/Dog/dog.848.jpg   \n",
      "  inflating: train/Dog/dog.849.jpg   \n",
      "  inflating: train/Dog/dog.85.jpg    \n",
      "  inflating: train/Dog/dog.850.jpg   \n",
      "  inflating: train/Dog/dog.851.jpg   \n",
      "  inflating: train/Dog/dog.852.jpg   \n",
      "  inflating: train/Dog/dog.853.jpg   \n",
      "  inflating: train/Dog/dog.854.jpg   \n",
      "  inflating: train/Dog/dog.855.jpg   \n",
      "  inflating: train/Dog/dog.856.jpg   \n",
      "  inflating: train/Dog/dog.857.jpg   \n",
      "  inflating: train/Dog/dog.858.jpg   \n",
      "  inflating: train/Dog/dog.859.jpg   \n",
      "  inflating: train/Dog/dog.86.jpg    \n",
      "  inflating: train/Dog/dog.860.jpg   \n",
      "  inflating: train/Dog/dog.861.jpg   \n",
      "  inflating: train/Dog/dog.862.jpg   \n",
      "  inflating: train/Dog/dog.863.jpg   \n",
      "  inflating: train/Dog/dog.864.jpg   \n",
      "  inflating: train/Dog/dog.865.jpg   \n",
      "  inflating: train/Dog/dog.866.jpg   \n",
      "  inflating: train/Dog/dog.867.jpg   \n",
      "  inflating: train/Dog/dog.868.jpg   \n",
      "  inflating: train/Dog/dog.869.jpg   \n",
      "  inflating: train/Dog/dog.87.jpg    \n",
      "  inflating: train/Dog/dog.870.jpg   \n",
      "  inflating: train/Dog/dog.871.jpg   \n",
      "  inflating: train/Dog/dog.872.jpg   \n",
      "  inflating: train/Dog/dog.873.jpg   \n",
      "  inflating: train/Dog/dog.874.jpg   \n",
      "  inflating: train/Dog/dog.875.jpg   \n",
      "  inflating: train/Dog/dog.876.jpg   \n",
      "  inflating: train/Dog/dog.877.jpg   \n",
      "  inflating: train/Dog/dog.878.jpg   \n",
      "  inflating: train/Dog/dog.879.jpg   \n",
      "  inflating: train/Dog/dog.88.jpg    \n",
      "  inflating: train/Dog/dog.880.jpg   \n",
      "  inflating: train/Dog/dog.881.jpg   \n",
      "  inflating: train/Dog/dog.882.jpg   \n",
      "  inflating: train/Dog/dog.883.jpg   \n",
      "  inflating: train/Dog/dog.884.jpg   \n",
      "  inflating: train/Dog/dog.885.jpg   \n",
      "  inflating: train/Dog/dog.886.jpg   \n",
      "  inflating: train/Dog/dog.887.jpg   \n",
      "  inflating: train/Dog/dog.888.jpg   \n",
      "  inflating: train/Dog/dog.889.jpg   \n",
      "  inflating: train/Dog/dog.89.jpg    \n",
      "  inflating: train/Dog/dog.890.jpg   \n",
      "  inflating: train/Dog/dog.891.jpg   \n",
      "  inflating: train/Dog/dog.892.jpg   \n",
      "  inflating: train/Dog/dog.893.jpg   \n",
      "  inflating: train/Dog/dog.894.jpg   \n",
      "  inflating: train/Dog/dog.895.jpg   \n",
      "  inflating: train/Dog/dog.896.jpg   \n",
      "  inflating: train/Dog/dog.897.jpg   \n",
      "  inflating: train/Dog/dog.898.jpg   \n",
      "  inflating: train/Dog/dog.9.jpg     \n",
      "  inflating: train/Dog/dog.90.jpg    \n",
      "  inflating: train/Dog/dog.91.jpg    \n",
      "  inflating: train/Dog/dog.92.jpg    \n",
      "  inflating: train/Dog/dog.93.jpg    \n",
      "  inflating: train/Dog/dog.936.jpg   \n",
      "  inflating: train/Dog/dog.937.jpg   \n",
      "  inflating: train/Dog/dog.938.jpg   \n",
      "  inflating: train/Dog/dog.939.jpg   \n",
      "  inflating: train/Dog/dog.94.jpg    \n",
      "  inflating: train/Dog/dog.940.jpg   \n",
      "  inflating: train/Dog/dog.941.jpg   \n",
      "  inflating: train/Dog/dog.942.jpg   \n",
      "  inflating: train/Dog/dog.943.jpg   \n",
      "  inflating: train/Dog/dog.944.jpg   \n",
      "  inflating: train/Dog/dog.945.jpg   \n",
      "  inflating: train/Dog/dog.946.jpg   \n",
      "  inflating: train/Dog/dog.947.jpg   \n",
      "  inflating: train/Dog/dog.948.jpg   \n",
      "  inflating: train/Dog/dog.949.jpg   \n",
      "  inflating: train/Dog/dog.95.jpg    \n",
      "  inflating: train/Dog/dog.950.jpg   \n",
      "  inflating: train/Dog/dog.951.jpg   \n",
      "  inflating: train/Dog/dog.952.jpg   \n",
      "  inflating: train/Dog/dog.953.jpg   \n",
      "  inflating: train/Dog/dog.954.jpg   \n",
      "  inflating: train/Dog/dog.955.jpg   \n",
      "  inflating: train/Dog/dog.956.jpg   \n",
      "  inflating: train/Dog/dog.957.jpg   \n",
      "  inflating: train/Dog/dog.958.jpg   \n",
      "  inflating: train/Dog/dog.959.jpg   \n",
      "  inflating: train/Dog/dog.96.jpg    \n",
      "  inflating: train/Dog/dog.960.jpg   \n",
      "  inflating: train/Dog/dog.961.jpg   \n",
      "  inflating: train/Dog/dog.962.jpg   \n",
      "  inflating: train/Dog/dog.963.jpg   \n",
      "  inflating: train/Dog/dog.964.jpg   \n",
      "  inflating: train/Dog/dog.965.jpg   \n",
      "  inflating: train/Dog/dog.966.jpg   \n",
      "  inflating: train/Dog/dog.967.jpg   \n",
      "  inflating: train/Dog/dog.968.jpg   \n",
      "  inflating: train/Dog/dog.969.jpg   \n",
      "  inflating: train/Dog/dog.97.jpg    \n",
      "  inflating: train/Dog/dog.970.jpg   \n",
      "  inflating: train/Dog/dog.971.jpg   \n",
      "  inflating: train/Dog/dog.972.jpg   \n",
      "  inflating: train/Dog/dog.973.jpg   \n",
      "  inflating: train/Dog/dog.974.jpg   \n",
      "  inflating: train/Dog/dog.975.jpg   \n",
      "  inflating: train/Dog/dog.976.jpg   \n",
      "  inflating: train/Dog/dog.977.jpg   \n",
      "  inflating: train/Dog/dog.978.jpg   \n",
      "  inflating: train/Dog/dog.979.jpg   \n",
      "  inflating: train/Dog/dog.98.jpg    \n",
      "  inflating: train/Dog/dog.980.jpg   \n",
      "  inflating: train/Dog/dog.981.jpg   \n",
      "  inflating: train/Dog/dog.982.jpg   \n",
      "  inflating: train/Dog/dog.983.jpg   \n",
      "  inflating: train/Dog/dog.984.jpg   \n",
      "  inflating: train/Dog/dog.985.jpg   \n",
      "  inflating: train/Dog/dog.986.jpg   \n",
      "  inflating: train/Dog/dog.987.jpg   \n",
      "  inflating: train/Dog/dog.988.jpg   \n",
      "  inflating: train/Dog/dog.989.jpg   \n",
      "  inflating: train/Dog/dog.99.jpg    \n",
      "  inflating: train/Dog/dog.990.jpg   \n",
      "  inflating: train/Dog/dog.991.jpg   \n",
      "  inflating: train/Dog/dog.992.jpg   \n",
      "  inflating: train/Dog/dog.993.jpg   \n",
      "  inflating: train/Dog/dog.994.jpg   \n",
      "  inflating: train/Dog/dog.995.jpg   \n",
      "  inflating: train/Dog/dog.996.jpg   \n",
      "  inflating: train/Dog/dog.997.jpg   \n",
      "  inflating: train/Dog/dog.998.jpg   \n",
      "  inflating: train/Dog/dog.999.jpg   \n",
      "   creating: validation/\n",
      "   creating: validation/Cat/\n",
      "  inflating: validation/Cat/cat.2407.jpg  \n",
      "  inflating: validation/Cat/cat.2408.jpg  \n",
      "  inflating: validation/Cat/cat.2409.jpg  \n",
      "  inflating: validation/Cat/cat.2410.jpg  \n",
      "  inflating: validation/Cat/cat.2411.jpg  \n",
      "  inflating: validation/Cat/cat.2412.jpg  \n",
      "  inflating: validation/Cat/cat.2413.jpg  \n",
      "  inflating: validation/Cat/cat.2414.jpg  \n",
      "  inflating: validation/Cat/cat.2415.jpg  \n",
      "  inflating: validation/Cat/cat.2416.jpg  \n",
      "  inflating: validation/Cat/cat.2417.jpg  \n",
      "  inflating: validation/Cat/cat.2418.jpg  \n",
      "  inflating: validation/Cat/cat.2419.jpg  \n",
      "  inflating: validation/Cat/cat.2420.jpg  \n",
      "  inflating: validation/Cat/cat.2421.jpg  \n",
      "  inflating: validation/Cat/cat.2422.jpg  \n",
      "  inflating: validation/Cat/cat.2423.jpg  \n",
      "  inflating: validation/Cat/cat.2424.jpg  \n",
      "  inflating: validation/Cat/cat.2425.jpg  \n",
      "  inflating: validation/Cat/cat.2426.jpg  \n",
      "  inflating: validation/Cat/cat.2427.jpg  \n",
      "  inflating: validation/Cat/cat.2428.jpg  \n",
      "  inflating: validation/Cat/cat.2429.jpg  \n",
      "  inflating: validation/Cat/cat.2430.jpg  \n",
      "  inflating: validation/Cat/cat.2431.jpg  \n",
      "  inflating: validation/Cat/cat.2432.jpg  \n",
      "  inflating: validation/Cat/cat.2433.jpg  \n",
      "  inflating: validation/Cat/cat.2434.jpg  \n",
      "  inflating: validation/Cat/cat.2435.jpg  \n",
      "   creating: validation/Dog/\n",
      "  inflating: validation/Dog/dog.2402.jpg  \n",
      "  inflating: validation/Dog/dog.2403.jpg  \n",
      "  inflating: validation/Dog/dog.2404.jpg  \n",
      "  inflating: validation/Dog/dog.2405.jpg  \n",
      "  inflating: validation/Dog/dog.2406.jpg  \n",
      "  inflating: validation/Dog/dog.2407.jpg  \n",
      "  inflating: validation/Dog/dog.2408.jpg  \n",
      "  inflating: validation/Dog/dog.2409.jpg  \n",
      "  inflating: validation/Dog/dog.2410.jpg  \n",
      "  inflating: validation/Dog/dog.2411.jpg  \n",
      "  inflating: validation/Dog/dog.2412.jpg  \n",
      "  inflating: validation/Dog/dog.2413.jpg  \n",
      "  inflating: validation/Dog/dog.2414.jpg  \n",
      "  inflating: validation/Dog/dog.2415.jpg  \n",
      "  inflating: validation/Dog/dog.2416.jpg  \n",
      "  inflating: validation/Dog/dog.2417.jpg  \n",
      "  inflating: validation/Dog/dog.2418.jpg  \n",
      "  inflating: validation/Dog/dog.2419.jpg  \n",
      "  inflating: validation/Dog/dog.2420.jpg  \n",
      "  inflating: validation/Dog/dog.2421.jpg  \n",
      "  inflating: validation/Dog/dog.2422.jpg  \n",
      "  inflating: validation/Dog/dog.2423.jpg  \n",
      "  inflating: validation/Dog/dog.2424.jpg  \n",
      "  inflating: validation/Dog/dog.2425.jpg  \n",
      "  inflating: validation/Dog/dog.2426.jpg  \n",
      "  inflating: validation/Dog/dog.2427.jpg  \n",
      "  inflating: validation/Dog/dog.2428.jpg  \n",
      "  inflating: validation/Dog/dog.2429.jpg  \n",
      "  inflating: validation/Dog/dog.2430.jpg  \n",
      "  inflating: validation/Dog/dog.2431.jpg  \n"
     ]
    }
   ],
   "source": [
    "!unzip catdog.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JE3kMaLwg39m"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F87r-cEqhz7N"
   },
   "outputs": [],
   "source": [
    "train_data_dir='/content/train'\n",
    "validation_data_dir='/content/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2o5y1R8Jh-CF"
   },
   "outputs": [],
   "source": [
    "num_train_samples=2000\n",
    "num_validation_samples=800\n",
    "epochs=5\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRk4jjusiHx2",
    "outputId": "655b6a2f-63dd-412c-aaf9-4a75d6679429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model=VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kUEIJrsig8b",
    "outputId": "aa10b272-8841-4b2d-a733-c5e1b1b6fb26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7d09d99b96f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d99b9600>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09dbdd0f40>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7d09d9a25510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d9a26f80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912c640>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7d09d912d630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d9a240a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912eaa0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912fa90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7d09d912e950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912f040>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912edd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d912e3e0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7d09d9142d70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d9a26ce0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d9143cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7d09d9142800>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7d09d914d240>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ujy9hqZzizZu"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "  layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-XBOajEXi52g"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cxX-is1YjP8M"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1Kna9bBjqfS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
